import os
import sys
import numpy as np
from PIL import Image
import gradio as gr
import torch
import time
from contextlib import nullcontext

# Configurer les chemins n√©cessaires
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

# Configuration optimis√©e pour RTX 4060
os.environ["GRADIO_ANALYTICS_ENABLED"] = "False"
os.environ["GRADIO_VERBOSE"] = "True"
os.environ["PYTHONUNBUFFERED"] = "1"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"

# Optimisation CUDA suppl√©mentaire pour RTX 4060
torch.backends.cudnn.benchmark = True
torch.cuda.empty_cache()  # Vider le cache CUDA au d√©marrage

# Fonction pour d√©tecter les capacit√©s GPU et ajuster les param√®tres
def get_gpu_info():
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        return gpu_name, gpu_memory_total
    return "CPU", 0

def detect_gpu_capacity():
    """D√©tecte les capacit√©s GPU et retourne les param√®tres optimaux"""
    if not torch.cuda.is_available():
        return {
            "width": 384, 
            "height": 512, 
            "precision": "float32", 
            "batch_size": 1,
            "steps_scale": 0.5
        }
    
    gpu_name, gpu_memory_total = get_gpu_info()
    
    # Optimisations sp√©cifiques pour RTX 4060
    if "RTX 4060" in gpu_name:
        if gpu_memory_total >= 8.0:  # 8GB VRAM
            return {
                "width": 768, 
                "height": 1024, 
                "precision": "bfloat16", 
                "batch_size": 1,
                "steps_scale": 0.8
            }
        else:  # < 8GB VRAM
            return {
                "width": 512, 
                "height": 768, 
                "precision": "float16", 
                "batch_size": 1,
                "steps_scale": 0.7
            }
    # Autres GPU NVIDIA avec beaucoup de VRAM
    elif gpu_memory_total >= 12.0:  # >= 12GB VRAM
        return {
            "width": 768, 
            "height": 1024, 
            "precision": "bfloat16" if torch.cuda.is_bf16_supported() else "float16", 
            "batch_size": 1,
            "steps_scale": 0.8
        }
    # GPU avec VRAM moyenne
    elif gpu_memory_total >= 8.0:  # >= 8GB VRAM
        return {
            "width": 768, 
            "height": 1024, 
            "precision": "float16", 
            "batch_size": 1,
            "steps_scale": 0.7
        }
    # GPU avec peu de VRAM
    elif gpu_memory_total >= 4.0:  # >= 4GB VRAM
        return {
            "width": 512, 
            "height": 768, 
            "precision": "float16", 
            "batch_size": 1,
            "steps_scale": 0.6
        }
    else:
        return {
            "width": 384, 
            "height": 512, 
            "precision": "float32", 
            "batch_size": 1,
            "steps_scale": 0.5
        }

# Param√®tres GPU optimaux bas√©s sur la d√©tection
gpu_params = detect_gpu_capacity()

print("\n===== D√âMARRAGE DE LEFFA ULTRA (MODE SIMPLIFI√â) =====")
print("L'application va d√©marrer et sera accessible √† l'adresse http://127.0.0.1:7860")
print("Ce mode est optimis√© pour l'essayage virtuel avec performances maximales sur GPU RTX 4060.")
print(f"R√©solution adapt√©e dynamiquement: {gpu_params['width']}x{gpu_params['height']}")
print(f"Pr√©cision num√©rique: {gpu_params['precision']}")

# Initialisation de l'interface Gradio simplifi√©e
try:
    print("Cr√©ation de l'interface Gradio simplifi√©e...")
    
    with gr.Blocks(title="Leffa Ultra - Mode Simplifi√©", css="footer {visibility: hidden}") as demo:
        gr.HTML(
            """
            <div style="text-align: center; max-width: 800px; margin: 0 auto;">
                <div>
                    <h1 style="font-weight: 900; font-size: 3rem; margin: 0.5rem 0;">üß• Leffa Ultra - Mode Simplifi√© üß•</h1>
                    <h2 style="font-weight: 450; font-size: 1rem; margin: 0.5rem 0;">Optimis√© pour RTX 4060</h2>
                </div>
                <p style="margin-bottom: 10px; font-size: 94%">
                    Cette version simplifi√©e de Leffa Ultra est en cours de configuration pour votre syst√®me.
                </p>
            </div>
            """
        )
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ‚ö†Ô∏è Mode Maintenance")
                gr.Markdown("""
                L'application Leffa Ultra est actuellement en mode maintenance.
                
                Nous rencontrons des probl√®mes avec certaines d√©pendances qui n√©cessitent des outils de compilation C++ (Visual Studio Build Tools).
                
                Pour r√©soudre ce probl√®me, vous pouvez:
                
                1. Installer Visual Studio Build Tools avec le composant "Outils de d√©veloppement C++"
                2. Utiliser une version pr√©compil√©e de l'application
                
                L'√©quipe travaille activement √† r√©soudre ce probl√®me pour vous offrir une exp√©rience optimale avec votre GPU RTX 4060.
                """)
                
                gpu_info = gr.Markdown(f"""
                **Informations GPU**
                - Dispositif: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}
                - M√©moire: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB si disponible
                - R√©solution configur√©e: {gpu_params['width']}x{gpu_params['height']}
                - Pr√©cision configur√©e: {gpu_params['precision']}
                """)
    
    print("\n===== INTERFACE LEFFA SIMPLIFI√âE PR√äTE =====")
    demo.launch(show_api=False)
    
except Exception as e:
    print(f"Erreur lors de l'initialisation: {e}")
    import traceback
    traceback.print_exc()
