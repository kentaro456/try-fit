import os
import sys
import numpy as np
from PIL import Image
import gradio as gr
import torch

# Configurer les chemins n√©cessaires
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)
sys.path.append(os.path.join(current_dir, '3rdparty'))
sys.path.append(os.path.join(current_dir, '3rdparty/densepose'))
sys.path.append(os.path.join(current_dir, '3rdparty/SCHP'))
sys.path.append(os.path.join(current_dir, '3rdparty/detectron2'))

# Configuration optimis√©e pour RTX 4060
os.environ["GRADIO_ANALYTICS_ENABLED"] = "False"
os.environ["GRADIO_VERBOSE"] = "True"
os.environ["PYTHONUNBUFFERED"] = "1"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"

# Optimisation CUDA suppl√©mentaire pour RTX 4060
torch.backends.cudnn.benchmark = True
torch.cuda.empty_cache()  # Vider le cache CUDA au d√©marrage

# Patch les fonctions non compatibles avec CUDA
import torchvision
from torchvision.ops.boxes import _box_inter_union, remove_small_boxes

def patched_batched_nms(boxes, scores, idxs, iou_threshold):
    """
    Performs non-maximum suppression in a batched fashion.
    Version modifi√©e qui √©vite l'erreur CUDA en utilisant le CPU uniquement pour la fonction NMS.
    """
    # D√©placer les donn√©es vers le CPU pour l'op√©ration NMS
    cpu_boxes = boxes.cpu()
    cpu_scores = scores.cpu()
    cpu_idxs = idxs.cpu()
    
    # Appliquer NMS sur CPU
    if boxes.numel() == 0:
        return torch.empty((0,), dtype=torch.int64, device=boxes.device)
    
    # strategy: in order to perform NMS independently per class,
    # we add an offset to each box based on its class. This offset ensures that
    # boxes from different classes do not overlap
    max_coordinate = cpu_boxes.max()
    offsets = cpu_idxs.to(torch.float) * (max_coordinate + 1)
    boxes_for_nms = cpu_boxes + offsets[:, None]
    
    # Utilise la version CPU de NMS
    cpu_keep = torch.ops.torchvision.nms(boxes_for_nms, cpu_scores, iou_threshold)
    
    # Renvoie les r√©sultats sur le m√™me device que l'entr√©e
    return cpu_keep.to(boxes.device)

# Sauvegarder la r√©f√©rence √† la fonction originale AVANT de la patcher
original_roi_align = torch.ops.torchvision.roi_align

# Patch pour l'op√©ration roi_align qui n'est pas compatible avec CUDA
def patched_roi_align(input, rois, spatial_scale, output_size_h, output_size_w, sampling_ratio, aligned):
    """
    Version modifi√©e de roi_align qui utilise le CPU puis renvoie les r√©sultats sur le m√™me device que l'entr√©e.
    """
    original_device = input.device
    
    # Si d√©j√† sur CPU, pas besoin de convertir
    if original_device.type == 'cpu':
        return original_roi_align(input, rois, spatial_scale, output_size_h, output_size_w, sampling_ratio, aligned)
    
    # D√©placer les donn√©es vers le CPU
    cpu_input = input.cpu()
    cpu_rois = rois.cpu()
    
    # Effectuer l'op√©ration roi_align sur CPU avec la fonction originale
    result = original_roi_align(
        cpu_input, 
        cpu_rois, 
        spatial_scale, 
        output_size_h, 
        output_size_w, 
        sampling_ratio, 
        aligned
    )
    
    # Renvoyer le r√©sultat sur le m√™me device que l'entr√©e
    return result.to(original_device)

# Patcher les fonctions dans torchvision
torchvision.ops.boxes._batched_nms_coordinate_trick = patched_batched_nms

# Rediriger les appels √† roi_align vers notre version patch√©e
torch.ops.torchvision.roi_align = patched_roi_align

print("\n===== D√âMARRAGE DE LEFFA ULTRA (MODE ESSAYAGE VIRTUEL OPTIMIS√â) =====")
print("L'application va d√©marrer et sera accessible √† l'adresse http://127.0.0.1:7860")
print("Ce mode est optimis√© pour l'essayage virtuel avec performances maximales sur GPU RTX 4060.")

# Import des modules n√©cessaires pour l'essayage virtuel
from huggingface_hub import snapshot_download
from leffa.transform import LeffaTransform
from leffa.model import LeffaModel
from leffa.inference import LeffaInference
from leffa_utils.garment_agnostic_mask_predictor import AutoMasker
from leffa_utils.densepose_predictor import DensePosePredictor
from leffa_utils.utils import resize_and_center, list_dir, get_agnostic_mask_hd, get_agnostic_mask_dc, preprocess_garment_image
from preprocess.humanparsing.run_parsing import Parsing
from preprocess.openpose.run_openpose import OpenPose

# T√©l√©chargement des checkpoints (uniquement ceux n√©cessaires pour l'essayage virtuel)
print("T√©l√©chargement des mod√®les minimaux requis...")
snapshot_download(repo_id="franciszzj/Leffa", 
                  local_dir="./ckpts", 
                  allow_patterns=[
                      "examples/**",
                      "stable-diffusion-inpainting/**",
                      "densepose/**", 
                      "schp/**", 
                      "humanparsing/**", 
                      "openpose/**",
                      "virtual_tryon.pth"
                  ])

class LeffaEssayagePredictor(object):
    def __init__(self):
        print("Initialisation des pr√©dicteurs (mode GPU optimis√©)...")
        
        # V√©rification de la disponibilit√© du GPU
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Utilisation du dispositif: {self.device}")
        
        # Afficher les informations GPU
        if self.device.type == 'cuda':
            print(f"GPU: {torch.cuda.get_device_name(0)}")
            print(f"M√©moire GPU totale: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        
        # Chargement des mod√®les essentiels uniquement
        self.mask_predictor = AutoMasker(
            densepose_path="./ckpts/densepose",
            schp_path="./ckpts/schp",
        )

        self.densepose_predictor = DensePosePredictor(
            config_path="./ckpts/densepose/densepose_rcnn_R_50_FPN_s1x.yaml",
            weights_path="./ckpts/densepose/model_final_162be9.pkl",
        )

        self.parsing = Parsing(
            atr_path="./ckpts/humanparsing/parsing_atr.onnx",
            lip_path="./ckpts/humanparsing/parsing_lip.onnx",
        )

        self.openpose = OpenPose(
            body_model_path="./ckpts/openpose/body_pose_model.pth",
        )

        # Uniquement le mod√®le HD pour l'essayage virtuel
        print("Chargement du mod√®le d'essayage virtuel...")
        vt_model_hd = LeffaModel(
            pretrained_model_name_or_path="./ckpts/stable-diffusion-inpainting",
            pretrained_model="./ckpts/virtual_tryon.pth",
            dtype="float16",  # Utiliser float16 pour le GPU
        )
        self.vt_inference_hd = LeffaInference(model=vt_model_hd)

    def leffa_predict_vt(self, 
                         src_image_path, 
                         ref_image_path, 
                         ref_acceleration=False, 
                         step=20,  # R√©duit √† 20 pour plus de rapidit√©
                         scale=3.0,  # Augment√© √† 3.0 pour meilleure qualit√©
                         seed=42, 
                         vt_model_type="viton_hd", 
                         vt_garment_type="upper_body", 
                         vt_repaint=False, 
                         preprocess_garment=False,
                         optimise_performance=True):  # Nouveau param√®tre pour optimiser les performances
        
        print(f"Traitement des images: {src_image_path} et {ref_image_path}")
        
        # Choisir la taille en fonction du type de v√™tement et de l'optimisation
        width, height = (768, 1024)  # Taille standard HD
        if optimise_performance:
            # R√©duire la taille des images pour les performances
            width, height = (512, 768) if vt_garment_type in ["upper_body", "lower_body"] else (640, 832)
        
        # Optimisation de la m√©moire GPU
        torch.cuda.empty_cache()
        
        # Ouvrir et redimensionner l'image source
        src_image = Image.open(src_image_path)
        src_image = resize_and_center(src_image, width, height)

        # Pr√©traiter l'image du v√™tement si n√©cessaire
        if preprocess_garment:
            if isinstance(ref_image_path, str) and ref_image_path.lower().endswith('.png'):
                ref_image = preprocess_garment_image(ref_image_path)
            else:
                raise ValueError("L'image du v√™tement doit √™tre au format PNG lorsque le pr√©traitement est activ√©.")
        else:
            ref_image = Image.open(ref_image_path)
            
        ref_image = resize_and_center(ref_image, width, height)
        src_image_array = np.array(src_image)
        
        # Pr√©paration des donn√©es pour l'essayage virtuel
        src_image = src_image.convert("RGB")
        
        # Taille adapt√©e pour le traitement initial
        parsing_size = (256, 352) if optimise_performance else (384, 512)
        model_parse, _ = self.parsing(src_image.resize(parsing_size))
        keypoints = self.openpose(src_image.resize(parsing_size))
        
        mask = get_agnostic_mask_hd(model_parse, keypoints, vt_garment_type)
        mask = mask.resize((width, height))
        
        # Pr√©paration de DensePose
        src_image_seg_array = self.densepose_predictor.predict_seg(src_image_array)[:, :, ::-1]
        src_image_seg = Image.fromarray(src_image_seg_array)
        densepose = src_image_seg
        
        # Transformation des donn√©es
        transform = LeffaTransform()
        data = {
            "src_image": [src_image],
            "ref_image": [ref_image],
            "mask": [mask],
            "densepose": [densepose],
        }
        data = transform(data)
        
        # Inf√©rence avec param√®tres adapt√©s au type de v√™tement
        # Ajuster le nombre d'√©tapes en fonction du type de v√™tement
        adjusted_steps = step
        if vt_garment_type == "dresses":
            # Les robes sont plus complexes et n√©cessitent plus d'√©tapes
            adjusted_steps = min(step + 5, 50)
        
        # Inf√©rence
        output = self.vt_inference_hd(
            data,
            ref_acceleration=ref_acceleration,
            num_inference_steps=adjusted_steps,
            guidance_scale=scale,
            seed=seed,
            repaint=vt_repaint,
        )
        
        gen_image = output["generated_image"][0]
        return np.array(gen_image), np.array(mask), np.array(densepose)

# Initialisation du pr√©dicteur
try:
    print("Cr√©ation du pr√©dicteur Leffa (mode essayage optimis√© pour RTX 4060)...")
    leffa_predictor = LeffaEssayagePredictor()
    
    # Pr√©paration des exemples
    example_dir = "./ckpts/examples"
    person1_images = list_dir(f"{example_dir}/person1")
    garment_images = list_dir(f"{example_dir}/garment")
    
    print("Cr√©ation de l'interface Gradio...")
    
    # Interface Gradio am√©lior√©e
    with gr.Blocks(theme=gr.themes.Default(primary_hue=gr.themes.colors.pink)) as demo:
        gr.Markdown("# üåü Leffa Ultra - Essayage Virtuel Optimis√© üåü")
        gr.Markdown("Application optimis√©e pour l'essayage virtuel de v√™tements sur GPU RTX 4060")
        
        with gr.Row():
            with gr.Column(scale=1):
                src_image = gr.Image(type="filepath", label="üì∑ Image de la personne")
                ref_image = gr.Image(type="filepath", label="üëï Image du v√™tement")
                
                with gr.Group():
                    gr.Markdown("### Type de v√™tement")
                    vt_garment_type = gr.Radio(
                        choices=[
                            "upper_body", 
                            "lower_body", 
                            "dresses"
                        ],
                        value="upper_body",
                        label="S√©lectionnez le type de v√™tement",
                        info="Haut du corps, bas du corps, ou robe compl√®te"
                    )
                
                with gr.Accordion("‚öôÔ∏è Param√®tres de qualit√©", open=True):
                    quality_preset = gr.Radio(
                        choices=["Rapide", "Standard", "Haute qualit√©"],
                        value="Standard",
                        label="Pr√©r√©glage de qualit√©",
                        info="Impact sur la vitesse et la qualit√© du rendu"
                    )
                    
                    optimise_performance = gr.Checkbox(
                        value=True,
                        label="Optimiser pour les performances",
                        info="Optimise la m√©moire et la vitesse de traitement"
                    )
                
                with gr.Accordion("üîß Param√®tres avanc√©s", open=False):
                    preprocess_garment = gr.Checkbox(
                        value=False,
                        label="Pr√©traiter l'image du v√™tement (PNG transparent uniquement)",
                        info="Utile pour les v√™tements avec fond transparent"
                    )
                    
                    ref_acceleration = gr.Checkbox(
                        value=True, 
                        label="Acc√©l√©ration r√©f√©rence",
                        info="Acc√©l√®re le processus de g√©n√©ration"
                    )
                    
                    vt_repaint = gr.Checkbox(
                        value=False, 
                        label="Repeindre",
                        info="Am√©liore la qualit√© mais ralentit le processus"
                    )
                    
                    step = gr.Slider(
                        minimum=1, 
                        maximum=50, 
                        value=20, 
                        step=1, 
                        label="Nombre d'√©tapes d'inf√©rence",
                        info="Plus d'√©tapes = meilleure qualit√© mais plus lent"
                    )
                    
                    scale = gr.Slider(
                        minimum=1, 
                        maximum=10, 
                        value=3.0, 
                        step=0.5, 
                        label="√âchelle de guidage",
                        info="Contr√¥le l'importance de l'image de r√©f√©rence"
                    )
                    
                    seed = gr.Number(
                        value=42, 
                        label="Graine al√©atoire",
                        info="Utiliser la m√™me valeur pour des r√©sultats reproductibles"
                    )
                
                predict_btn = gr.Button("‚ú® Essayer le v√™tement", variant="primary", size="lg")
                
            with gr.Column(scale=1):
                result_image = gr.Image(label="üñºÔ∏è R√©sultat")
                with gr.Accordion("Visualisations techniques", open=False):
                    mask_image = gr.Image(label="Masque")
                    densepose_image = gr.Image(label="DensePose")
        
        # Logique pour ajuster les param√®tres en fonction du pr√©r√©glage de qualit√©
        def update_quality_params(preset):
            if preset == "Rapide":
                return 10, 2.0, True, False, True
            elif preset == "Haute qualit√©":
                return 35, 4.0, False, True, False
            else:  # Standard
                return 20, 3.0, True, False, True
        
        # Mettre √† jour les param√®tres lorsque le pr√©r√©glage change
        quality_preset.change(
            fn=update_quality_params,
            inputs=[quality_preset],
            outputs=[step, scale, ref_acceleration, vt_repaint, optimise_performance]
        )
        
        # Fonction de pr√©diction mise √† jour
        def predict_wrapper(src_image, ref_image, quality_preset, vt_garment_type, preprocess_garment, 
                           ref_acceleration, vt_repaint, step, scale, seed, optimise_performance):
            # Obtenir les param√®tres du pr√©r√©glage
            params = update_quality_params(quality_preset)
            
            # Utiliser les param√®tres du pr√©r√©glage √† moins qu'ils n'aient √©t√© modifi√©s manuellement
            if step == params[0]:
                step = params[0]
            if scale == params[1]:
                scale = params[1]
            if ref_acceleration == params[2]:
                ref_acceleration = params[2]
            if vt_repaint == params[3]:
                vt_repaint = params[3]
            if optimise_performance == params[4]:
                optimise_performance = params[4]
            
            # Appeler la fonction de pr√©diction
            gen_image, mask, densepose = leffa_predictor.leffa_predict_vt(
                src_image, ref_image, ref_acceleration, step, scale, seed, 
                "viton_hd", vt_garment_type, vt_repaint, preprocess_garment, optimise_performance
            )
            
            return gen_image, mask, densepose
        
        # Connecter les entr√©es et sorties
        predict_btn.click(
            fn=predict_wrapper,
            inputs=[
                src_image, ref_image, quality_preset, vt_garment_type, preprocess_garment,
                ref_acceleration, vt_repaint, step, scale, seed, optimise_performance
            ],
            outputs=[result_image, mask_image, densepose_image]
        )
        
        # Exemples am√©lior√©s
        gr.Examples(
            examples=[
                [person1_images[0], garment_images[0], "Standard", "upper_body", False, True, False, 20, 3.0, 42, True],
                [person1_images[1], garment_images[1], "Haute qualit√©", "dresses", False, False, True, 35, 4.0, 42, False],
            ],
            inputs=[
                src_image, ref_image, quality_preset, vt_garment_type, preprocess_garment,
                ref_acceleration, vt_repaint, step, scale, seed, optimise_performance
            ],
            outputs=[result_image, mask_image, densepose_image],
        )
    
    print("\n===== INTERFACE LEFFA PR√äTE (MODE ESSAYAGE UNIQUEMENT AVEC GPU) =====")
    demo.launch(server_name="127.0.0.1", server_port=7860)

except Exception as e:
    import traceback
    print("Une erreur est survenue lors de l'initialisation:")
    print(traceback.format_exc())
    print(e)
